{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12888527,"sourceType":"datasetVersion","datasetId":8154425},{"sourceId":12986009,"sourceType":"datasetVersion","datasetId":8219488}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  Tiny LLM on NASA Space Data  \n\n## *Introduction*  \nIn this notebook, we will train a **very small Language Model (LLM)** on **NASA space-related datasets**.  \nThe goal is educational – to understand the basic steps of preparing data, building a small model, training it, and generating text.  \n\nWe will use:  \n* **eva.csv** → Extravehicular activities (spacewalks)  \n* **neo.csv / nearest-earth-objects.csv** → Near-Earth Objects data  \n* **cleaned_5250.csv** → Cleaned dataset for training  \n\n### *What we will do:*  \n1. Load and explore the dataset  \n2. Prepare text data for training  \n3. Tokenize the text  \n4. Define a very small LLM (RNN-based)  \n5. Train the model  \n6. Generate sample text about space  \n\n---\n","metadata":{}},{"cell_type":"markdown","source":"#  Import Libraries and Setup\n\nWe will use **PyTorch** for building the model.  \nThe Kaggle notebook may run on CPU or GPU, so we will detect the device first.\n","metadata":{}},{"cell_type":"code","source":"# !pip uninstall -y torch torchvision torchaudio functorch\n# !pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --quiet\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os, sys\n# os.kill(os.getpid(), 9)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# print(torch.__version__)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.func import grad_and_value\nimport pandas as pd\nimport numpy as np\nimport random\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  Load and Explore Dataset  \n\nWe will start with **eva.csv** (Extravehicular Activities - spacewalks).  \nSteps:  \n* Read the CSV file  \n* Display a few rows  \n* Understand what columns exist (we will later convert them into text for training)  \n","metadata":{}},{"cell_type":"code","source":"# Load eva.csv file\neva_df = pd.read_csv(\"/kaggle/input/nasa-training/eva.csv\")\n\n# Show first few rows\neva_df.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  Merge All CSVs into a Single Text Corpus  \n\nWe have multiple datasets:  \n* eva.csv → Extravehicular Activities (spacewalks)  \n* neo.csv → Near-Earth Objects  \n* nearest-earth-objects.csv → Historical NEO data  \n* cleaned_5250.csv & neo_v2.csv → Cleaned datasets  \n\nWe will:  \n1. Load each CSV file.  \n2. Convert each row into **string text**.  \n3. Merge everything into **one big text corpus** for training our LLM.  \n","metadata":{}},{"cell_type":"code","source":"# List of all CSV files (NASA + natural language)\ncsv_files = [\n    # \"/kaggle/input/nasa-training/eva.csv\",\n    # \"/kaggle/input/nasa-training/neo.csv\",\n    # \"/kaggle/input/nasa-training/nearest-earth-objects(1910-2024).csv\",\n    # \"/kaggle/input/nasa-training/cleaned_5250.csv\",\n    # \"/kaggle/input/nasa-training/neo_v2.csv\",\n    \"/kaggle/input/created-data/space_dataset_merged.csv\" # new natural-language dataset\n]\n\ntext_corpus = \"\"\n\nfor file in csv_files:\n    try:\n        df = pd.read_csv(file)\n        text_corpus += df.astype(str).apply(\" \".join, axis=1).str.cat(sep=\" \") + \" \"\n    except Exception as e:\n        print(f\"Error reading {file}: {e}\")\n\nprint(text_corpus[:500])\nprint(\"\\nCorpus length:\", len(text_corpus))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  Character-Level Tokenization  \n\nWe will:  \n1. Get all unique characters from the text corpus.  \n2. Assign each character an **integer ID**.  \n3. Convert the text corpus into a list of integers (tokens).  \n","metadata":{}},{"cell_type":"code","source":"chars = sorted(list(set(text_corpus)))\nvocab_size = len(chars)\n\nstoi = {ch: i for i, ch in enumerate(chars)} \nitos = {i: ch for i, ch in enumerate(chars)}  \n\ndef encode(text):\n    return [stoi[ch] for ch in text]\n\ndef decode(tokens):\n    return \"\".join([itos[i] for i in tokens])\n\ntokens = encode(text_corpus)\nprint(\"Vocabulary size:\", vocab_size)\nprint(\"Sample tokens:\", tokens[:50])\nprint(\"Decoded back:\", decode(tokens[:50]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  Create Dataset and DataLoader\n\n* We will create fixed-length context windows (`block_size`) from the token sequence.\n* Each training sample: input = tokens[i : i+block_size], target = tokens[i+1 : i+1+block_size]\n* We'll use a PyTorch Dataset + DataLoader for batching.\n","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nblock_size = 64  \nbatch_size = 32\ntokens_tensor = torch.tensor(tokens, dtype=torch.long)\n\nclass CharDataset(Dataset):\n    def __init__(self, data, block_size):\n        self.data = data\n        self.block_size = block_size\n    def __len__(self):\n        return max(0, (len(self.data) - 1) // self.block_size)\n    def __getitem__(self, idx):\n        start = idx * self.block_size\n        x = self.data[start : start + self.block_size]\n        y = self.data[start + 1 : start + 1 + self.block_size]\n        if x.size(0) < self.block_size:\n            pad_len = self.block_size - x.size(0)\n            x = torch.cat([x, torch.zeros(pad_len, dtype=torch.long)])\n            y = torch.cat([y, torch.zeros(pad_len, dtype=torch.long)])\n        return x, y\n\ndataset = CharDataset(tokens_tensor, block_size)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n\nprint(\"Dataset samples:\", len(dataset))\nbatch_x, batch_y = next(iter(dataloader))\nprint(\"Batch shapes:\", batch_x.shape, batch_y.shape)  \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  Tiny Transformer LM\n\n* Architecture components:\n  * token embedding + positional embeddings\n  * small `TransformerEncoder` stack with causal masking\n  * linear head to predict next token logits\n* Keep sizes small for quick training on Kaggle.\n","metadata":{}},{"cell_type":"code","source":"import math\nimport torch.nn as nn\nimport torch.nn.functional as F\nd_model = 128   \nnhead = 4        \nn_layers = 2    \ndim_feedforward = 256  \ndropout = 0.1\n\ndef generate_causal_mask(sz, device):\n    mask = torch.triu(torch.ones(sz, sz, device=device) * float('-inf'), diagonal=1)\n    return mask  \n\nclass TinyTransformerLM(nn.Module):\n    def __init__(self, vocab_size, block_size, d_model=128, nhead=4, n_layers=2, dim_feedforward=256, dropout=0.1):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.block_size = block_size\n        self.tok_emb = nn.Embedding(vocab_size, d_model)\n        self.pos_emb = nn.Embedding(block_size, d_model)\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, activation='relu')\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n        self.ln_f = nn.LayerNorm(d_model)\n        self.head = nn.Linear(d_model, vocab_size)\n\n    def forward(self, idx):\n        B, T = idx.shape\n        assert T <= self.block_size, \"T must be <= block_size\"\n        positions = torch.arange(T, device=idx.device).unsqueeze(0).expand(B, T)  \n        x = self.tok_emb(idx) + self.pos_emb(positions)\n\n        x = x.transpose(0, 1)  \n        src_mask = generate_causal_mask(T, idx.device)  \n        x = self.transformer(x, mask=src_mask)  \n        x = x.transpose(0, 1)  \n        x = self.ln_f(x)\n        logits = self.head(x) \n        return logits\n\n    @torch.no_grad()\n    def generate(self, idx, max_new_tokens=100, temperature=1.0):\n        for _ in range(max_new_tokens):\n            t = idx.shape[1]\n            start = max(0, t - self.block_size)\n            cur = idx[:, start:] \n            logits = self.forward(cur)  \n            logits = logits[:, -1, :] / temperature \n            probs = F.softmax(logits, dim=-1) \n            next_token = torch.multinomial(probs, num_samples=1)  \n            idx = torch.cat([idx, next_token], dim=1)\n        return idx\n\n# instantiate\nmodel = TinyTransformerLM(vocab_size=vocab_size, block_size=block_size, d_model=d_model, nhead=nhead, n_layers=n_layers, dim_feedforward=dim_feedforward, dropout=dropout).to(device)\nprint(\"Model parameters:\", sum(p.numel() for p in model.parameters()))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  Training Loop\n\n* Use CrossEntropyLoss (applies softmax + log-loss).\n* Keep epochs small (e.g., `epochs = 5`) for demonstration; adjust as needed.\n* Print loss every few steps.\n","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\nfrom tqdm import tqdm\nfrom torch.func import grad, grad_and_value\n\nepochs = 20\nlr = 3e-4\noptimizer = optim.AdamW(model.parameters(), lr=lr)\ncriterion = nn.CrossEntropyLoss()\n\nmodel.train()\nfor epoch in range(1, epochs + 1):\n    running_loss = 0.0\n    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch}\")\n    for i, (x, y) in pbar:\n        x = x.to(device)\n        y = y.to(device)\n        optimizer.zero_grad()\n        logits = model(x)\n        loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        if (i + 1) % 10 == 0:\n            pbar.set_postfix(loss=running_loss / (i + 1))\n    avg_loss = running_loss / len(dataloader)\n    print(f\"Epoch {epoch} finished — avg loss: {avg_loss:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), \"model_weights_only_text.pth\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model, \"full_model.pth\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  Generate text from the trained model\n\n* Provide a short prompt (string), encode it, and let the model autoregressively generate new tokens.\n* We'll decode tokens back to characters for readability.\n","metadata":{}},{"cell_type":"code","source":"model.eval()\n\ndef generate_from_prompt(prompt, max_new_tokens=200, temperature=1.0):\n    prompt_tokens = torch.tensor([encode(prompt)], dtype=torch.long).to(device) \n    out = model.generate(prompt_tokens, max_new_tokens=max_new_tokens, temperature=temperature)  \n    out_list = out[0].tolist()\n    return decode(out_list)\n\n\nprint(\n    \"\"\" Welcome, Traveler of the Void.  \nI am ORION, keeper of celestial pathways.  \nThrough me, the nebulae of knowledge shall unfold,  \nand the horizons of the unknown shall awaken.  \n\nDeclare your course… or fade beyond with 'exit'.  \n\"\"\")\n\nwhile True:\n    prompt = input(\"Herald of the heavens, utter your will — the universe awaits. \")\n    if prompt.lower() in [\"exit\", \"quit\", \"stop\"]:\n        print(\"Stellar-GPT signing off. May your course be steady and your stars eternal.\")\n        break\n    \n    generated = generate_from_prompt(prompt, max_new_tokens=300, temperature=0.9)\n    print(\"\\n\"+ generated)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}