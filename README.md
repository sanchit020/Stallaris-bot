Implemented a character-level Transformer-based Language Model (LLM) in PyTorch, trained from scratch on a custom text corpus.

Key Features:

  Built a custom tokenizer, dataset loader, and causal attention mask

  Designed a TinyTransformer architecture with multi-head self-attention

  Trained the model on text data to learn language patterns from scratch

  Generated meaningful text outputs using greedy decoding & sampling

Tech Stack: Python, PyTorch, Transformers, NLP
